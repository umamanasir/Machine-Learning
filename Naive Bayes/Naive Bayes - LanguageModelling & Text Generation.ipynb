{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM9OZqepqHZa"
      },
      "source": [
        "# Programming Assignment 2: Naive Bayes\n",
        "## Part 1: Language Modelling and Text Generation\n",
        "\n",
        "#### Name: Umama Nasir Abbasi\n",
        "#### Roll Number: 23100265\n",
        "\n",
        "### Instructions\n",
        "*   In this part of the assignment you will be implementing an n-gram model for text-generation.\n",
        "*   Your code must be in the Python programming language.\n",
        "*   You are encouraged to use procedural programming and throughly comment your code.\n",
        "*   For Part 1, in addition to standard libraries i.e. numpy, pandas, regex, matplotlib and scipy, you can use [UrduHack](https://docs.urduhack.com/en/stable/index.html) for tokenization, and [NLKT](https://www.nltk.org/) for training your n-grams. However, no other machine learning toolkits or libraries are allowed.\n",
        "*   **Carefully read the submission guidelines, plagiarism and late days policy.**\n",
        "\n",
        "### Submission Guidelines\n",
        "Submit your code both as notebook file (.ipynb) and python script (.py) as individual files on LMS. Name both files as RollNumber_PA2_PartNum, i.e. this part should be named as `2xxxxxxx_PA4_1`. If you don’t know how to save .ipynb as .py see [this](https://i.stack.imgur.com/L1rQH.png). Failing to submit any one of them might result in the reduction of marks. All cells **MUST** be run to get credit.\n",
        "\n",
        "### Plagiarism Policy\n",
        "The code **MUST** be done independently. Any plagiarism or cheating of work from others or the internet will be immediately referred to the DC. If you are confused about what constitutes plagiarism, it is **YOUR** responsibility to consult with the instructor or the TA in a timely manner. No “after the fact” negotiations will be possible. The only way to guarantee that you do not lose marks is **DO NOT LOOK AT ANYONE ELSE'S CODE NOR DISCUSS IT WITH THEM**.\n",
        "\n",
        "### Late Days Policy\n",
        "\n",
        "The deadline for the assignment is final. However, in order to accommodate all the 11th-\n",
        "hour issues, there is a late submission policy i.e. you can submit your assignment within\n",
        "3 days after the deadline with a 25% deduction each day.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upeglgh9qHZ7"
      },
      "source": [
        "### Introduction\n",
        "An n-gram is a contiguous sequence of n words. For example \"Machine\" is a unigram, \"Machine Learning\" is a bigram and \"Machine Learning PA2\" is a trigram. In language modeling, n-gram models are probabilistic models of text that use word dependencies and context to predict the likelihood of occurence of an n-gram, i.e. predicting the nth word in an n-gram based on the previous n-1 words:\n",
        "$$\n",
        "P(ngram) =  P(word|context) = P(x^{n}|x^{n-1},...,x^{1})\n",
        "$$\n",
        "One use of the predictions made by such a model is text generation. In this part you will be training your own n-gram model and using it to generate text after learning from the provided Urdu short stories. \n",
        "<br><br>\n",
        "For additional details of the working of n-gram models, you can also consult [Chapter 3](https://web.stanford.edu/~jurafsky/slp3/3.pdf) of the Speech and Language Processing book as and references.\n",
        "\n",
        "\n",
        "### Dataset\n",
        "You will be using the Urdu short stories by Patras Bukhari given in the folder `Urdu Short Stories` in the PA2 zip file for the purposes of this part of the assignment. This contains 6 stories of varying lengths which will serve as inputs for your n-gram model. \n",
        "You're required to implement an n-gram model that uses the given stories to generate Urdu text that mimics the input stories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usL8pAqqqHZ-"
      },
      "source": [
        "Start by importing all required libraries here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install urduhack\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIYNzwmcqSn3",
        "outputId": "0886329a-91cf-428d-c5fb-ead21415956d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urduhack\n",
            "  Downloading urduhack-1.1.1-py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 18.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow-datasets~=3.1\n",
            "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 32.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from urduhack) (2022.6.2)\n",
            "Collecting tf2crf\n",
            "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: Click~=7.1 in /usr/local/lib/python3.7/dist-packages (from urduhack) (7.1.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.23.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.11.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (0.3.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (3.19.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (4.64.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (22.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2022.9.24)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets~=3.1->urduhack) (1.57.0)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf2crf->urduhack) (2.9.2)\n",
            "Collecting tensorflow-addons>=0.8.2\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.50.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.12)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (21.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (14.0.6)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.3.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (4.1.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (57.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.27.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->tf2crf->urduhack) (0.38.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.1.0->tf2crf->urduhack) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (2.14.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.1.0->tf2crf->urduhack) (3.2.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.8.2->tf2crf->urduhack) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow>=2.1.0->tf2crf->urduhack) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons, tf2crf, tensorflow-datasets, urduhack\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.6.0\n",
            "    Uninstalling tensorflow-datasets-4.6.0:\n",
            "      Successfully uninstalled tensorflow-datasets-4.6.0\n",
            "Successfully installed tensorflow-addons-0.18.0 tensorflow-datasets-3.2.1 tf2crf-0.1.33 urduhack-1.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5p6fdbxqHZ_"
      },
      "outputs": [],
      "source": [
        "# import all required libraries here\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import urduhack\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqfKfVK5r3a6",
        "outputId": "966f7122-c236-4144-ea7c-a72cb839c0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anN7w0DSqHaD"
      },
      "source": [
        "### 1.1 - Loading and Preprocessing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHe93pDAqHaG"
      },
      "source": [
        "Read in the short story files given and tokenize the text to be preprocessed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYIkU_BsqHaI"
      },
      "outputs": [],
      "source": [
        "#  code here\n",
        "short_story = glob.glob('/content/drive/MyDrive/Fall2022-2023/CS535ML/PA4/DataP1/*'+ '.txt')\n",
        "processed = []\n",
        "\n",
        "for story in range(0, len(short_story)):\n",
        "  content = open(short_story[story], encoding='utf-8')\n",
        "  c = content.read()\n",
        "  processed.append(c)\n",
        "  content.close()\n",
        "\n",
        "def getTokens(story):\n",
        "  tokens = story.split(' ')\n",
        "  return tokens\n",
        "\n",
        "\n",
        "total_tokens = []\n",
        "for story in processed: \n",
        "  total_tokens = total_tokens + getTokens(story)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBSJp6gCqHaO"
      },
      "source": [
        "Preprocess the tokenized data. Go through the data and use your own discretion to decide on what kind of pre-processing might be required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZnixPlDqHaR",
        "outputId": "1ca48c88-ad90-4e0f-bee1-c87af52e61fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16033,)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# code here\n",
        "for i in range(len(total_tokens)):\n",
        "  total_tokens[i] = urduhack.normalization.normalize(total_tokens[i])\n",
        "  total_tokens[i] = urduhack.preprocessing.remove_punctuation(total_tokens[i])\n",
        "  total_tokens[i] = urduhack.preprocessing.remove_accents(total_tokens[i])\n",
        "  total_tokens[i] = urduhack.preprocessing.replace_numbers(total_tokens[i])\n",
        "  total_tokens[i] = urduhack.preprocessing.replace_currency_symbols(total_tokens[i])\n",
        "  total_tokens[i] = urduhack.preprocessing.replace_phone_numbers(total_tokens[i])\n",
        "  total_tokens[i] = urduhack.preprocessing.replace_urls(total_tokens[i])\n",
        "  total_tokens[i] = urduhack.preprocessing.replace_emails(total_tokens[i])\n",
        "  total_tokens[i] = urduhack.preprocessing.normalize_whitespace(total_tokens[i])\n",
        "\n",
        "\n",
        "\n",
        "total_tokens = np.array(total_tokens)\n",
        "total_tokens.shape  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmsY2xQ5qHaS"
      },
      "source": [
        "### 1.2 - Creating Unigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRLr-9qCqHaT"
      },
      "source": [
        "Start by training a unigram model. For a unigram model, the n-gram probability is approximated by probability of the word in the unigram, as the model assumes independence:\n",
        "\n",
        "$$\n",
        "P(word) = \\frac{n}{N}\n",
        "$$\n",
        "\n",
        "where n = count of the word in the corpus and N = total number of words in the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsacPa9BqHaU"
      },
      "source": [
        "Generate a list of unigrams. Print the first 10 unigrams obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhqwLet8qHaW",
        "outputId": "4ff3337c-6855-4e57-87ec-5dc465452f24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ہم']\n",
            "['نے']\n",
            "['کالج']\n",
            "['میں']\n",
            "['تعلیم']\n",
            "['تو']\n",
            "['ضرور']\n",
            "['پائی']\n",
            "['اور']\n",
            "['رفتہ']\n"
          ]
        }
      ],
      "source": [
        "# code here\n",
        "u_grams = nltk.ngrams(total_tokens,1)\n",
        "ugram_array = np.array(list(u_grams))\n",
        "for i in range(10):\n",
        "  print(ugram_array[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBFNLQ9sqHaX"
      },
      "source": [
        "Find the probabilities for each unique unigram. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2143z-SqHaY"
      },
      "outputs": [],
      "source": [
        "# code here\n",
        "dict_ugram = {}\n",
        "words, occurance = np.unique(ugram_array, return_counts = True)\n",
        "p = []\n",
        "total_occ = occurance.sum()\n",
        "for i in range(occurance.shape[0]):\n",
        "  dict_ugram[words[i]] = occurance[i]/total_occ\n",
        "  p.append(occurance[i]/total_occ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "708E2iRJqHab"
      },
      "source": [
        "### 1.3 - Creating Bigrams\n",
        "Now train a bigram model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkCidQb5qHac"
      },
      "source": [
        "Generate a list of bigrams. Print the first 10 bigrams obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHsGDhpPqHad",
        "outputId": "64dc1efd-2cca-43b0-8039-99198ed6cf1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ہم' 'نے']\n",
            "['نے' 'کالج']\n",
            "['کالج' 'میں']\n",
            "['میں' 'تعلیم']\n",
            "['تعلیم' 'تو']\n",
            "['تو' 'ضرور']\n",
            "['ضرور' 'پائی']\n",
            "['پائی' 'اور']\n",
            "['اور' 'رفتہ']\n",
            "['رفتہ' 'رفتہ']\n"
          ]
        }
      ],
      "source": [
        "# code here\n",
        "bigram = nltk.bigrams(total_tokens)\n",
        "bigram = np.array(list(bigram))\n",
        "for i in range(10):\n",
        "  print(bigram[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VCUWB1PqHad"
      },
      "source": [
        "Find the probabilities for each unique bigram. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Klpka78eqHae"
      },
      "outputs": [],
      "source": [
        "# code here\n",
        "dict_bigram = {}\n",
        "words_bigram, occurance_bigram = np.unique(bigram, return_counts = True,axis=0)\n",
        "p_bigram = []\n",
        "total_occ = occurance_bigram.sum()\n",
        "for i in range(occurance_bigram.shape[0]):\n",
        "  w = tuple(words_bigram[i])\n",
        "  dict_bigram[w] = occurance_bigram[i]/total_occ\n",
        "  p_bigram.append(occurance_bigram[i]/total_occ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkmDZ3KCqHae"
      },
      "source": [
        "### 1.4 - Creating Trigrams\n",
        "Lastly train a trigram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_IU2udQqHaf"
      },
      "source": [
        "Generate a list of trigrams. Print the first 10 trigrams obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBEw2Z3oqHaf",
        "outputId": "bf988a06-aa84-4cd5-a2d3-68cd58516dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ہم' 'نے' 'کالج']\n",
            "['نے' 'کالج' 'میں']\n",
            "['کالج' 'میں' 'تعلیم']\n",
            "['میں' 'تعلیم' 'تو']\n",
            "['تعلیم' 'تو' 'ضرور']\n",
            "['تو' 'ضرور' 'پائی']\n",
            "['ضرور' 'پائی' 'اور']\n",
            "['پائی' 'اور' 'رفتہ']\n",
            "['اور' 'رفتہ' 'رفتہ']\n",
            "['رفتہ' 'رفتہ' 'بیاے']\n"
          ]
        }
      ],
      "source": [
        "# code here\n",
        "trigram = nltk.ngrams(total_tokens,3)\n",
        "trigram = np.array(list(trigram))\n",
        "for i in range(10):\n",
        "  print(trigram[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ29-WrQqHag"
      },
      "source": [
        "Find the probabilities for each unique trigram. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIuRQ01iqHai"
      },
      "outputs": [],
      "source": [
        "# code here\n",
        "dict_trigram = {}\n",
        "words_trigram, occurance_trigram = np.unique(trigram, return_counts = True,axis=0)\n",
        "p_trigram = []\n",
        "total_occ = occurance_trigram.sum()\n",
        "for i in range(occurance_trigram.shape[0]):\n",
        "  w = tuple(words_trigram[i])\n",
        "  dict_trigram[w] = occurance_trigram[i]/total_occ\n",
        "  p_trigram.append(occurance_trigram[i]/total_occ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBJLSSkwqHai"
      },
      "source": [
        "### 1.5 - Generating Text\n",
        "Generate a paragraph with ten sentences each containing 9-15 words (pick the length of the sentence randomly within this range) using you language model. Start with trigrams, use back-off technique (i.e. use n-1 gram) if a token is not available. \n",
        "\n",
        "For each word prediction, get top 5 most probabale words using the n-gram model and then pick the next word randomly from within these. This is being done to avoid excessive repetitive sequences in your generated text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_655YKiqHaj"
      },
      "outputs": [],
      "source": [
        "def get_bigrams(w):\n",
        "  # print(w)\n",
        "  bigrams_total = []\n",
        "  final = {}\n",
        "  bigrams = (dict_bigram.keys())\n",
        "  # print(\"l: \", bigrams)\n",
        "  for i in bigrams:\n",
        "    # print('i: ', i)\n",
        "    if i[0] == w:\n",
        "      bigrams_total.append(i)\n",
        "  for k,v in dict_bigram.items():\n",
        "    for j in bigrams_total:\n",
        "      if j == k :\n",
        "        final[j] = v\n",
        "  if len(final.keys()) < 5:\n",
        "    return \"\"  \n",
        "  else:\n",
        "    keys = sorted(final, key = final.get, reverse=True)\n",
        "    keys = np.array(keys[0:5])\n",
        "    rows = np.random.randint(5,size=1)\n",
        "    w = keys[rows[0]]\n",
        "    return tuple(w)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trigrams(w):\n",
        "  trigrams_total =[]\n",
        "  final = {}\n",
        "  trigrams = (dict_trigram.keys())\n",
        "  for i in trigrams:\n",
        "    if i[0] == w[0] and i[1] == w[1]:\n",
        "      trigrams_total.append(i)\n",
        "  for key,value in dict_trigram.items():\n",
        "    for j in trigrams_total:\n",
        "      if j == key:\n",
        "        final[j] = value\n",
        "  if (len(final.keys())) <5:\n",
        "    return \"\"\n",
        "  else:\n",
        "    k = sorted(final, key = final.get,reverse=True)\n",
        "    k = np.array(k[0:5])\n",
        "    row = np.random.randint(5,size=1)\n",
        "    w = k[row[0]]\n",
        "    return tuple(w)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AyDIbYjuyYGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_sentences=[]\n",
        "for i in range(10):\n",
        "  sent = ''\n",
        "  word = np.random.choice(ugram_array.flatten(),1)\n",
        "  sent=word[0]\n",
        "  while len(sent.split(\" \")) < 15: \n",
        "    temp =sent.split()\n",
        "    bi_gram = get_bigrams(temp[-1])\n",
        "    if bi_gram !=\"\":\n",
        "      sent = sent + \" \" + bi_gram[1]\n",
        "      temp1 = sent.split()\n",
        "      tuple_temp = tuple([temp1[-2], temp1[-1]])\n",
        "      trigram = get_trigrams(tuple_temp)\n",
        "      if trigram!= \"\":\n",
        "        sent = sent + \" \" + trigram[2]\n",
        "        while trigram != \"\" and len(sent.split(\" \")) < 15:\n",
        "          temp3 = sent.split()\n",
        "          tuple_temp = tuple([temp3[-2], temp3[-1]])\n",
        "          trigram = get_trigrams(tuple_temp)\n",
        "          if trigram != \"\":\n",
        "            sent = sent + \" \" + trigram[2]\n",
        "    elif bi_gram == \"\":\n",
        "      uni = np.random.choice(ugram_array.flatten(),1)\n",
        "      sent = sent+ \" \" + uni[0]\n",
        "  final_sentences.append(sent)  \n",
        "  print(sent)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqwr95xKzcIN",
        "outputId": "08b28929-bba1-4a97-e486-c422da15c41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تھیٹروں تو ضرور حاصل تھا وہ تو اب میں سے آسمان تین اصحاب چنانچہ اس\n",
            "ہوا ہے اس سے آپ کو ان کو باہر جاتے تاریخ اور اس میں جو\n",
            "اور ان میں نے اپنے کام آتے یہ ہوا تو ہم اس میں ضرور ایسی\n",
            "طور وہ یہ کہ فارسی میں ایک آزادی ہیں اس کی باتیں ذہن گی آپ\n",
            "اس میں سفر بعض ایسے لوگ اجی انجمن تو ضرور مضمر بس صاحب کے متعلق میں\n",
            "آواز اور آپ کی آزاد کی ایک اور ان بیرون مصوری ذرا تاریک دلآویزیوں کی\n",
            "سینماسے اگر آپ کے سامنے آبیٹھے کیوں نہ کوئی بات ایسی اچھی فلم کون ہمارے\n",
            "تمہارے ہاتھ سے کام نہ کوئی ایسی بےمروتی سے پہلے مرزا سے کام لیا اور\n",
            "بیٹھے لیکن جب آپ کو ان کی تسلی آدھ منٹ بس یہی تو میں بائیں\n",
            "ایک آدھ کروٹ اس میں سما خیال سے آپ کیونکر لیڈروں تاریخی اور آپ کی\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined = ''\n",
        "for i in final_sentences:\n",
        "  combined = combined + \" \" + i\n",
        "  "
      ],
      "metadata": {
        "id": "y-YerWme488q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined"
      ],
      "metadata": {
        "id": "W_QVPJ3t5EFT",
        "outputId": "c3ab62d8-6c1c-4d34-e241-b9c7a43840a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' تھیٹروں تو ضرور حاصل تھا وہ تو اب میں سے آسمان تین اصحاب چنانچہ اس ہوا ہے اس سے آپ کو ان کو باہر جاتے تاریخ اور اس میں جو اور ان میں نے اپنے کام آتے یہ ہوا تو ہم اس میں ضرور ایسی طور وہ یہ کہ فارسی میں ایک آزادی ہیں اس کی باتیں ذہن گی آپ اس میں سفر بعض ایسے لوگ اجی انجمن تو ضرور مضمر بس صاحب کے متعلق میں آواز اور آپ کی آزاد کی ایک اور ان بیرون مصوری ذرا تاریک دلآویزیوں کی سینماسے اگر آپ کے سامنے آبیٹھے کیوں نہ کوئی بات ایسی اچھی فلم کون ہمارے تمہارے ہاتھ سے کام نہ کوئی ایسی بےمروتی سے پہلے مرزا سے کام لیا اور بیٹھے لیکن جب آپ کو ان کی تسلی آدھ منٹ بس یہی تو میں بائیں ایک آدھ کروٹ اس میں سما خیال سے آپ کیونکر لیڈروں تاریخی اور آپ کی'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G0OTXmdqHak"
      },
      "source": [
        "### 1.6 - Discussion and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pDBANnsqHal"
      },
      "source": [
        "- Analyze the text generated, and mention 3 distinct observations. Also compare it with the input text and how different it is and why might that be.\n",
        "- Is going upto n=3 enough? What do you think would be a good value of n and why?\n",
        "\n",
        "Answer here:\n",
        "\n",
        "1- The phrases make sense however the sentences as a whole make no sense at all. alot of conjunctions are used in the generated text such as iss, hai, koo etc.\n",
        "\n",
        "\n",
        "2- We can go beyong n=3 for more choesive sentences however it might lead to overfitting.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1w8G9j3Rszo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "970a2a4939579a4c22872227820a264ec023ee5692739211cbaca24386397975"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}